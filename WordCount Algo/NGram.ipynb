{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Step 1- Merge all 4 PDF\n",
    "\n",
    "#Use TIKA Lib for merging file\n",
    "import sys\n",
    "from tika import parser\n",
    "\n",
    "parsed = parser.from_file('mergedPDF.pdf')\n",
    "\n",
    "#Pass output in text file\n",
    "sys.stdout = open('o.txt','wt',encoding='utf-8')\n",
    "print(parsed [\"content\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Step2 - remove special characters from text file\n",
    "\n",
    "#Used regular expression\n",
    "import re\n",
    "\n",
    "string = open('o.txt', encoding= 'utf-8').read()\n",
    "new_str = re.sub('[^a-zA-Z0-9\\n\\.]', ' ', string)\n",
    "open('oupdated.txt','w').write(new_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Step 3- #Cleanning data \n",
    "file__1 = open('rstopwords.txt', 'r')\n",
    "file_2 = open('clean.txt', 'w')\n",
    "file_1_cont = file1.readlines()\n",
    "\n",
    "#remove stopwords using stopword file\n",
    "for line in file1_cont:\n",
    "    line.replace('.', ' ')\n",
    "    file2.write(line)remove stopwords using given file of stopwords\n",
    "stoplist = []\n",
    "file_1 = open('l.txt','r')\n",
    "file_2 = open('stopwords.txt','r')\n",
    "file_3 = open('cleanedv1.txt','a')\n",
    "\n",
    "#Add the words \n",
    "for line in file2:\n",
    "    w = line.split()\n",
    "    for word in w:\n",
    "        stoplist.append(word)\n",
    "\n",
    "for line in file1:\n",
    "    w = line.split()\n",
    "    for word in w:\n",
    "        if word in stoplist: continue\n",
    "        else: \n",
    "            file3.write(word)\n",
    "            file3.write(' ')\n",
    "\n",
    "#end \n",
    "file1.close()\n",
    "file2.close()\n",
    "file3.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Cleanning dots \n",
    "file1 = open('rstopwords.txt', 'r')\n",
    "file2 = open('clean.txt', 'w')\n",
    "file1_cont = file1.readlines()\n",
    "for line in file1_cont:\n",
    "    line.replace('.', ' ')\n",
    "    file2.write(line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Lower the cases\n",
    "with open('rstopwords.txt', 'r') as fileinput:\n",
    "   for line in fileinput:\n",
    "       line = line.lower()#Remove all the unwanted words \n",
    "#cleanning word using NLTK\n",
    "import io \n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords \n",
    "from nltk.tokenize import word_tokenize \n",
    "\n",
    "#word_tokenize accepts a string as an input, not a file. \n",
    "stop_words = set(stopwords.words('english')) \n",
    "file1 = open(\"cleanedv1.txt\") \n",
    "line = file1.read()\n",
    "\n",
    "# Use this to read file content as a stream: \n",
    "words = line.split() \n",
    "for r in words: \n",
    "\tif not r in stop_words: \n",
    "\t\tappendFile = open('filteredtext.txt','a') \n",
    "\t\tappendFile.write(\" \"+r) \n",
    "\t\tappendFile.close() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import nlp libraries\n",
    "import string\n",
    "import collections\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.util import ngrams\n",
    "import nltk, re, string, collections\n",
    "from nltk.util import ngrams # function for making ngrams\n",
    "\n",
    "# set nlp variables\n",
    "english_stops = stopwords.words('english')\n",
    "lemmatizer = nltk.stem.WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Convert Text file into list\n",
    "with open(\"/Users/shira/OneDrive/Desktop/ML/Case Study1/filteredtext.txt\", \"r\", encoding='utf-8') as file:\n",
    "    text = file.read()\n",
    "    tokens = nltk.word_tokenize(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create function to clean text , Lowercases, takes out punct and short strings\n",
    "def cleanning_tokens(tokens):\n",
    "    return [token.lower() for token in tokens if (token not in string.punctuation) and \n",
    "                   (token.lower() not in english_stops) and len(token) > 2]\n",
    "\n",
    "#create function to Lemmatize like remove plurals(play, plays)- it will save- play\n",
    "def lemmatize(tokens):\n",
    "    return [lemmatizer.lemmatize(token) for token in tokens]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Clean words\n",
    "clean = cleanning_tokens(tokens)\n",
    "#print(clean)\n",
    "lemmi = lemmatize(clean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Count ngram freqeuencies \n",
    "def counting_ngrams(tokens,n):\n",
    "    n_grams_count = ngrams(tokens, n)\n",
    "    ngramFrequency = collections.Counter(n_grams_count)\n",
    "    ngramFrequency = ngramFrequency.most_common()\n",
    "    return ngram_freq\n",
    "\n",
    "#Convert it into dic\n",
    "def ngram_to_dict(ngramFrequency):\n",
    "    l = []\n",
    "    for t in ngramFrequency:\n",
    "        l.append((' '.join(t[0]),t[1]))\n",
    "    return dict(l)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "str"
      ]
     },
     "execution_count": 198,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [],
   "source": [
    "# count word and ngram frequency\n",
    "word_freq = counting_ngrams(lemmi, 1)\n",
    "bigram_freq = counting_ngrams(lemmi, 2)\n",
    "#trigram_freq = counting_ngrams(lemmi, 3)\n",
    "#ngramFrequency = bigram_freq + trigram_freq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [],
   "source": [
    "#change to directory\n",
    "word_freq = ngram_to_dict(word_freq)\n",
    "bigram_freq = ngram_to_dict(bigram_freq)\n",
    "#trigram_freq = ngram_to_dict(trigram_freq)\n",
    "#ngramFrequency = ngram_to_dict(ngram_freq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Take output into text format\n",
    "import sys\n",
    "sys.stdout = open('bigram_.txt','wt',encoding='utf-8')\n",
    "print(bigram_freq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Convert list into dataframe\n",
    "import pandas as pd\n",
    "\n",
    "dfObj = pd.DataFrame(list(bbigram_freq.items()),columns=['bigram_Words', 'count'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Convert dataframe into csv file\n",
    "dfObj.to_csv(\"output.csv\", encoding='utf-8', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
